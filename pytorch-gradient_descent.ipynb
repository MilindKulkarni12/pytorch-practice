{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor([2., 4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "print(X)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(y, y_pred):\n",
    "    return  ((y - y_pred)**2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True) 0.01 50\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "alpha = 0.01\n",
    "iterations = 50\n",
    "print(w, alpha, iterations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MulBackward0>)\n",
      "epoch 0, w: 0.300, loss: 30.000000\n",
      "epoch 1, w: 0.555, loss: 21.674999\n",
      "epoch 2, w: 0.772, loss: 15.660188\n",
      "epoch 3, w: 0.956, loss: 11.314487\n",
      "epoch 4, w: 1.113, loss: 8.174717\n",
      "epoch 5, w: 1.246, loss: 5.906232\n",
      "epoch 6, w: 1.359, loss: 4.267253\n",
      "epoch 7, w: 1.455, loss: 3.083090\n",
      "epoch 8, w: 1.537, loss: 2.227532\n",
      "epoch 9, w: 1.606, loss: 1.609392\n",
      "epoch 10, w: 1.665, loss: 1.162786\n",
      "epoch 11, w: 1.716, loss: 0.840112\n",
      "epoch 12, w: 1.758, loss: 0.606981\n",
      "epoch 13, w: 1.794, loss: 0.438544\n",
      "epoch 14, w: 1.825, loss: 0.316848\n",
      "epoch 15, w: 1.851, loss: 0.228923\n",
      "epoch 16, w: 1.874, loss: 0.165397\n",
      "epoch 17, w: 1.893, loss: 0.119499\n",
      "epoch 18, w: 1.909, loss: 0.086338\n",
      "epoch 19, w: 1.922, loss: 0.062379\n",
      "epoch 20, w: 1.934, loss: 0.045069\n",
      "epoch 21, w: 1.944, loss: 0.032562\n",
      "epoch 22, w: 1.952, loss: 0.023526\n",
      "epoch 23, w: 1.960, loss: 0.016998\n",
      "epoch 24, w: 1.966, loss: 0.012281\n",
      "epoch 25, w: 1.971, loss: 0.008873\n",
      "epoch 26, w: 1.975, loss: 0.006411\n",
      "epoch 27, w: 1.979, loss: 0.004632\n",
      "epoch 28, w: 1.982, loss: 0.003346\n",
      "epoch 29, w: 1.985, loss: 0.002418\n",
      "epoch 30, w: 1.987, loss: 0.001747\n",
      "epoch 31, w: 1.989, loss: 0.001262\n",
      "epoch 32, w: 1.991, loss: 0.000912\n",
      "epoch 33, w: 1.992, loss: 0.000659\n",
      "epoch 34, w: 1.993, loss: 0.000476\n",
      "epoch 35, w: 1.994, loss: 0.000344\n",
      "epoch 36, w: 1.995, loss: 0.000248\n",
      "epoch 37, w: 1.996, loss: 0.000180\n",
      "epoch 38, w: 1.996, loss: 0.000130\n",
      "epoch 39, w: 1.997, loss: 0.000094\n",
      "epoch 40, w: 1.997, loss: 0.000068\n",
      "epoch 41, w: 1.998, loss: 0.000049\n",
      "epoch 42, w: 1.998, loss: 0.000035\n",
      "epoch 43, w: 1.998, loss: 0.000026\n",
      "epoch 44, w: 1.999, loss: 0.000018\n",
      "epoch 45, w: 1.999, loss: 0.000013\n",
      "epoch 46, w: 1.999, loss: 0.000010\n",
      "epoch 47, w: 1.999, loss: 0.000007\n",
      "epoch 48, w: 1.999, loss: 0.000005\n",
      "epoch 49, w: 1.999, loss: 0.000004\n",
      "tensor(9.9970, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(forward(5))\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    # predict\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # calculate loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # gradient descent => backward pass in pytorch\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= alpha * w.grad\n",
    "\n",
    "    # reset gradients\n",
    "    w.grad.zero_()\n",
    "    print(f\"epoch {epoch}, w: {w:.3f}, loss: {l:5f}\")\n",
    "\n",
    "print(forward(5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent using PyTorch Neural Network Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Type 1: Use optimizer for loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor([2., 4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "print(X)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True) 0.01 50\n",
      "MSELoss() SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "alpha = 0.01\n",
    "iterations = 50\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=alpha)\n",
    "print(w, alpha, iterations)\n",
    "print(loss, optimizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MulBackward0>)\n",
      "epoch 0, w: 0.300, loss: 30.000000\n",
      "epoch 1, w: 0.555, loss: 21.674999\n",
      "epoch 2, w: 0.772, loss: 15.660188\n",
      "epoch 3, w: 0.956, loss: 11.314487\n",
      "epoch 4, w: 1.113, loss: 8.174717\n",
      "epoch 5, w: 1.246, loss: 5.906232\n",
      "epoch 6, w: 1.359, loss: 4.267253\n",
      "epoch 7, w: 1.455, loss: 3.083090\n",
      "epoch 8, w: 1.537, loss: 2.227532\n",
      "epoch 9, w: 1.606, loss: 1.609392\n",
      "epoch 10, w: 1.665, loss: 1.162786\n",
      "epoch 11, w: 1.716, loss: 0.840112\n",
      "epoch 12, w: 1.758, loss: 0.606981\n",
      "epoch 13, w: 1.794, loss: 0.438544\n",
      "epoch 14, w: 1.825, loss: 0.316848\n",
      "epoch 15, w: 1.851, loss: 0.228923\n",
      "epoch 16, w: 1.874, loss: 0.165397\n",
      "epoch 17, w: 1.893, loss: 0.119499\n",
      "epoch 18, w: 1.909, loss: 0.086338\n",
      "epoch 19, w: 1.922, loss: 0.062379\n",
      "epoch 20, w: 1.934, loss: 0.045069\n",
      "epoch 21, w: 1.944, loss: 0.032562\n",
      "epoch 22, w: 1.952, loss: 0.023526\n",
      "epoch 23, w: 1.960, loss: 0.016998\n",
      "epoch 24, w: 1.966, loss: 0.012281\n",
      "epoch 25, w: 1.971, loss: 0.008873\n",
      "epoch 26, w: 1.975, loss: 0.006411\n",
      "epoch 27, w: 1.979, loss: 0.004632\n",
      "epoch 28, w: 1.982, loss: 0.003346\n",
      "epoch 29, w: 1.985, loss: 0.002418\n",
      "epoch 30, w: 1.987, loss: 0.001747\n",
      "epoch 31, w: 1.989, loss: 0.001262\n",
      "epoch 32, w: 1.991, loss: 0.000912\n",
      "epoch 33, w: 1.992, loss: 0.000659\n",
      "epoch 34, w: 1.993, loss: 0.000476\n",
      "epoch 35, w: 1.994, loss: 0.000344\n",
      "epoch 36, w: 1.995, loss: 0.000248\n",
      "epoch 37, w: 1.996, loss: 0.000180\n",
      "epoch 38, w: 1.996, loss: 0.000130\n",
      "epoch 39, w: 1.997, loss: 0.000094\n",
      "epoch 40, w: 1.997, loss: 0.000068\n",
      "epoch 41, w: 1.998, loss: 0.000049\n",
      "epoch 42, w: 1.998, loss: 0.000035\n",
      "epoch 43, w: 1.998, loss: 0.000026\n",
      "epoch 44, w: 1.999, loss: 0.000018\n",
      "epoch 45, w: 1.999, loss: 0.000013\n",
      "epoch 46, w: 1.999, loss: 0.000010\n",
      "epoch 47, w: 1.999, loss: 0.000007\n",
      "epoch 48, w: 1.999, loss: 0.000005\n",
      "epoch 49, w: 1.999, loss: 0.000004\n",
      "tensor(9.9970, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(forward(5))\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    # predict\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # calculate loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # gradient descent => backward pass in pytorch\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset gradients\n",
    "    w.grad.zero_()\n",
    "    print(f\"epoch {epoch}, w: {w:.3f}, loss: {l:5f}\")\n",
    "\n",
    "print(forward(5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Type 2: Use PyTorch Neural Networks Module for model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]]) torch.Size([4, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.],\n",
      "        [8.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(X, X.shape)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "print(X_test, X_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "input_size, output_size = n_features, n_features\n",
    "model = nn.Linear(input_size, output_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True) 0.01 100\n",
      "MSELoss() SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "alpha = 0.01\n",
    "iterations = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=alpha)\n",
    "print(w, alpha, iterations)\n",
    "print(loss, optimizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4587202072143555\n",
      "epoch 0, w: 1.020, loss: 14.510107\n",
      "epoch 1, w: 1.401, loss: 10.069775\n",
      "epoch 2, w: 1.893, loss: 4.031501\n",
      "epoch 3, w: 2.411, loss: 0.260543\n",
      "epoch 4, w: 2.871, loss: 1.170763\n",
      "epoch 5, w: 3.194, loss: 6.179425\n",
      "epoch 6, w: 3.327, loss: 12.080225\n",
      "epoch 7, w: 3.247, loss: 15.095757\n",
      "epoch 8, w: 2.968, loss: 13.295565\n",
      "epoch 9, w: 2.536, loss: 7.831866\n",
      "epoch 10, w: 2.022, loss: 2.202020\n",
      "epoch 11, w: 1.512, loss: 0.009722\n",
      "epoch 12, w: 1.091, loss: 2.658171\n",
      "epoch 13, w: 0.830, loss: 8.451839\n",
      "epoch 14, w: 0.770, loss: 13.681831\n",
      "epoch 15, w: 0.923, loss: 15.000084\n",
      "epoch 16, w: 1.261, loss: 11.562565\n",
      "epoch 17, w: 1.728, loss: 5.569572\n",
      "epoch 18, w: 2.246, loss: 0.857254\n",
      "epoch 19, w: 2.728, loss: 0.441964\n",
      "epoch 20, w: 3.092, loss: 4.589386\n",
      "epoch 21, w: 3.277, loss: 10.644478\n",
      "epoch 22, w: 3.253, loss: 14.731043\n",
      "epoch 23, w: 3.022, loss: 14.233019\n",
      "epoch 24, w: 2.622, loss: 9.469100\n",
      "epoch 25, w: 2.121, loss: 3.488750\n",
      "epoch 26, w: 1.600, loss: 0.120121\n",
      "epoch 27, w: 1.147, loss: 1.519531\n",
      "epoch 28, w: 0.836, loss: 6.791140\n",
      "epoch 29, w: 0.720, loss: 12.560402\n",
      "epoch 30, w: 0.816, loss: 15.134223\n",
      "epoch 31, w: 1.108, loss: 12.865042\n",
      "epoch 32, w: 1.548, loss: 7.205480\n",
      "epoch 33, w: 2.060, loss: 1.778473\n",
      "epoch 34, w: 2.559, loss: 0.058106\n",
      "epoch 35, w: 2.962, loss: 3.145742\n",
      "epoch 36, w: 3.200, loss: 9.064991\n",
      "epoch 37, w: 3.232, loss: 14.026869\n",
      "epoch 38, w: 3.055, loss: 14.855253\n",
      "epoch 39, w: 2.695, loss: 11.020012\n",
      "epoch 40, w: 2.214, loss: 4.976367\n",
      "epoch 41, w: 1.690, loss: 0.593226\n",
      "epoch 42, w: 1.212, loss: 0.676551\n",
      "epoch 43, w: 0.858, loss: 5.173177\n",
      "epoch 44, w: 0.688, loss: 11.204836\n",
      "epoch 45, w: 0.729, loss: 14.910639\n",
      "epoch 46, w: 0.974, loss: 13.918543\n",
      "epoch 47, w: 1.382, loss: 8.863792\n",
      "epoch 48, w: 1.885, loss: 2.982256\n",
      "epoch 49, w: 2.398, loss: 0.039052\n",
      "epoch 50, w: 2.836, loss: 1.918363\n",
      "epoch 51, w: 3.125, loss: 7.417306\n",
      "epoch 52, w: 3.216, loss: 13.015934\n",
      "epoch 53, w: 3.095, loss: 15.130477\n",
      "epoch 54, w: 2.782, loss: 12.407427\n",
      "epoch 55, w: 2.328, loss: 6.589962\n",
      "epoch 56, w: 1.810, loss: 1.402080\n",
      "epoch 57, w: 1.314, loss: 0.164745\n",
      "epoch 58, w: 0.922, loss: 3.670023\n",
      "epoch 59, w: 0.701, loss: 9.674058\n",
      "epoch 60, w: 0.688, loss: 14.333436\n",
      "epoch 61, w: 0.884, loss: 14.665487\n",
      "epoch 62, w: 1.257, loss: 10.457586\n",
      "epoch 63, w: 1.745, loss: 4.403267\n",
      "epoch 64, w: 2.267, loss: 0.378001\n",
      "epoch 65, w: 2.735, loss: 0.958382\n",
      "epoch 66, w: 3.072, loss: 5.772757\n",
      "epoch 67, w: 3.222, loss: 11.739147\n",
      "epoch 68, w: 3.159, loss: 15.038113\n",
      "epoch 69, w: 2.896, loss: 13.557716\n",
      "epoch 70, w: 2.475, loss: 8.245450\n",
      "epoch 71, w: 1.969, loss: 2.501691\n",
      "epoch 72, w: 1.460, loss: 0.003043\n",
      "epoch 73, w: 1.036, loss: 2.348799\n",
      "epoch 74, w: 0.767, loss: 8.037190\n",
      "epoch 75, w: 0.699, loss: 13.426718\n",
      "epoch 76, w: 0.843, loss: 15.067204\n",
      "epoch 77, w: 1.175, loss: 11.908360\n",
      "epoch 78, w: 1.642, loss: 5.972125\n",
      "epoch 79, w: 2.165, loss: 1.058342\n",
      "epoch 80, w: 2.658, loss: 0.312367\n",
      "epoch 81, w: 3.038, loss: 4.211616\n",
      "epoch 82, w: 3.243, loss: 10.259956\n",
      "epoch 83, w: 3.239, loss: 14.585569\n",
      "epoch 84, w: 3.027, loss: 14.419426\n",
      "epoch 85, w: 2.644, loss: 9.867845\n",
      "epoch 86, w: 2.153, loss: 3.844414\n",
      "epoch 87, w: 1.637, loss: 0.204941\n",
      "epoch 88, w: 1.183, loss: 1.279185\n",
      "epoch 89, w: 0.868, loss: 6.379521\n",
      "epoch 90, w: 0.744, loss: 12.241110\n",
      "epoch 91, w: 0.832, loss: 15.111826\n",
      "epoch 92, w: 1.119, loss: 13.154116\n",
      "epoch 93, w: 1.557, loss: 7.621280\n",
      "epoch 94, w: 2.073, loss: 2.055189\n",
      "epoch 95, w: 2.582, loss: 0.019018\n",
      "epoch 96, w: 2.998, loss: 2.816331\n",
      "epoch 97, w: 3.254, loss: 8.656630\n",
      "epoch 98, w: 3.307, loss: 13.801508\n",
      "epoch 99, w: 3.147, loss: 14.957718\n",
      "15.989214897155762\n"
     ]
    }
   ],
   "source": [
    "print(model(X_test).item())\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    # predict\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # calculate loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # gradient descent => backward pass in pytorch\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    [w, b] = model.parameters()\n",
    "    print(f\"epoch {epoch}, w: {w[0][0].item():.3f}, loss: {l:5f}\")\n",
    "\n",
    "print(model(X_test).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
